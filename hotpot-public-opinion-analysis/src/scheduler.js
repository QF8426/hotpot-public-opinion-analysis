// src/scheduler.js

const cron = require('node-cron');
const fs   = require('fs');
const path = require('path');

const { connect, saveTopics } = require('./db');
const { parseAll } = require('./index');
const { cron: CRON_EXPR }       = require('../config/config.json');

// â€”â€” ç¡®ä¿ data ç›®å½•å­˜åœ¨ â€”â€”
const DATA_DIR = path.resolve(__dirname, '../data');
if (!fs.existsSync(DATA_DIR)) fs.mkdirSync(DATA_DIR, { recursive: true });

// â€”â€” å¼•å…¥çˆ¬è™«æ¨¡å— â€”â€”
const scrapers = [
  require('./scrapers/weibo'),
  require('./scrapers/bilibili'),
  require('./scrapers/douyin'),
  // require('./scrapers/xiaohongshu'),
];

/**
 * æ‰§è¡Œä¸€æ¬¡æŠ“å–â€”è§£æâ€”å­˜å‚¨æµç¨‹
 */
async function job() {
  const timestamp = new Date().toISOString();
  console.log(`ğŸ•’ Aggregate scrape at ${timestamp}`);

  try {
    // å¹¶è¡Œè°ƒç”¨çˆ¬è™«ï¼Œå•ä¸ªå¤±è´¥ä¸å½±å“æ•´ä½“
    const settled = await Promise.allSettled(scrapers.map(fn => fn()));
    const raws    = settled
      .filter(r => r.status === 'fulfilled')
      .map(r => r.value);

    console.log(`âœ” Scraped raw counts: [${raws.map(arr => arr.length).join(', ')}]`);

    // è§£ææ ‡å‡†åŒ–
    const parsed = parseAll(raws);
    console.log(`â„¹ Parsed total items: ${parsed.length}`);

    // è¿æ¥æ•°æ®åº“å¹¶å»é‡å†™å…¥
    await connect();
    await saveTopics(parsed);
    console.log('âœ… Data written to MongoDB (via saveTopics)');

    // æœ¬åœ° JSON å¤‡ä»½
    const filename = `all-${timestamp.slice(0,16).replace(/[:T]/g,'-')}.json`;
    const outfile  = path.join(DATA_DIR, filename);
    fs.writeFileSync(outfile, JSON.stringify(parsed, null, 2), 'utf8');
    console.log(`âœ… Backup: ${filename}`);
  } catch (err) {
    console.error('âŒ Aggregate job error:', err);
  }
}

// å¯åŠ¨æ—¶å…ˆè·‘ä¸€æ¬¡
job();

// æŒ‰ cron è¡¨è¾¾å¼å®šæ—¶æ‰§è¡Œ
cron.schedule(CRON_EXPR, job);
